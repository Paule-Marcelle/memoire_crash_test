# -*- coding: utf-8 -*-
"""Reddit_prepocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJWew8dwy95NBqUWc21T4W_KpdYsOl7U

### <font color='#00d2d3'> Importer les packages
"""

# Importer les packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('stopwords') # Télécharger le package stopwords
nltk.download('wordnet')
from nltk.corpus import stopwords # Importer le package stopwords

"""### <font color='#00d2d3'> Importer et lire les données"""

# Importer les données
from google.colab import drive
drive.mount('/content/drive')

# Afficher les données
data=pd.read_csv('/content/drive/MyDrive/Future_Intern/reddit_data.csv')
data

"""- 1 : Negative
- 0 : Positive
"""

# Dimension des données
data.shape

"""### <font color='#00d2d3'> Pré-traitement des données"""

print(data['selftext'].apply(type).value_counts())

"""#### <font color='#00d2d3'> Pré-traitement des données textuelles"""

# Afficher la première ligne de la colonne text
data['title'][0]

# Afficher la deuxième ligne de la colonne text
data['selftext'][0]

"""###### <font color='#1dd1a1'> Commencer le nettoyage des tweets

"""

# Fonction supprimant les links
def remove_links(title):
    title= re.sub(r'http:?//\S+ | https:?//\S+','',title)
    return title

def remove_links2(selftext):
    if isinstance(selftext, str):
        return re.sub(r'http:?//\S+ | https:?//\S+','',selftext)
    return selftext

# Fonction supprimant les @username
def remove_users(title):
    title = re.sub(r'@[\w\-._]+','',title)
    return title

def remove_users2(selftext):
    if isinstance(selftext, str):
        # Supprime les mentions utilisateurs (exemple : @username)
        return re.sub(r"@\w+", "", selftext)
    return selftext

# Supprimer les adresses emails
def email_address(text):
  title= re.sub(r'@[\w\-._]+','',text)
  return title

# Supprimer les adresses emails
def email_address2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne de caractères
        # Supprime les adresses email
        return re.sub(r'@[\w\-._]+', '', selftext)
    return selftext

# Exemple
tweet ='My email adress is the following : abdoulwahabdiall@gmail.com'
tweet = email_address(tweet)

!pip install contractions

import contractions
# Fonction étendant les contractions
def contraction(title):
    expanded_all = []
    for word in title.split():
        expanded_all.append(contractions.fix(word)) # utiliser la fonction fix de contractions

    expand = ' '.join(expanded_all)
    return expand

def contraction2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        expanded_all = [contractions.fix(word) for word in selftext.split()]
        return ' '.join(expanded_all)
    return selftext

# Exemple
tweet ="That's the right thing to do"
tweet = contraction(tweet)
tweet

# Supprimer html caractères
def clean_html(text):
   title = re.sub(r'&\w+','',text)
   return title

def clean_html2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        return re.sub(r'&\w+;', '', selftext)
    return selftext

# Exemple
tweet ='#world What a shame &amp'
tweet = clean_html(tweet)
tweet

# Remplacer tout ce qui n'est chaines de caractères alphabétiques et espace par ' '
def alpha_b(text):
   title = re.sub(r'^a-zA-Z\s]+',' ',text)
   return title

def alpha_b2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        # Supprimer tout sauf les lettres (a-z, A-Z) et les espaces
        return re.sub(r'[^a-zA-Z\s]+', ' ', selftext)
    return selftext

# Exemple
tweet = 'My year of birth is 2050 #happy'
tweet = alpha_b(tweet)
tweet

# Fonction remplaçant les espaces multiples et convertissant majuscules en minuscules
def lower(text):
    title = re.sub(r'\s(2,)',' ' ,text)
    return title

def lower2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        return selftext.lower()
    return selftext

# Supprimer les espaces en début et fin de tweet
def clean_space(text):
    title = re.sub(r'^\s|\s$',' ' ,text)
    return title

def clean_space2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        # Supprimer les espaces en début (^\\s) et en fin (\\s$) de chaîne
        return re.sub(r'^\s+|\s+$', '', selftext)
    return selftext

# Fonction supprimant les stopwords
def remove_stopwords(text):
    Stopwords = stopwords.words('english')
    title= ' '.join([word for word in text.split() if word not in Stopwords])
    return title

def remove_stopwords2(selftext):
    # Charger la liste des mots vides (stopwords)
    Stopwords = set(stopwords.words('english'))

    # Vérifier si l'entrée est une chaîne
    if isinstance(selftext, str):
        # Supprimer les mots vides
        return ' '.join([word for word in selftext.split() if word.lower() not in Stopwords])
    return selftext

# Exemple
tweet = 'Life is a mess for some people'
tweet = remove_stopwords(tweet)
tweet

# Lemmatization
from nltk.stem import WordNetLemmatizer
lemma=WordNetLemmatizer()
def lem_sw(text):
    title = [lemma.lemmatize(word) for word in text.split()]
    title = " ".join(title)
    return title

def lem_sw2(selftext):
    if isinstance(selftext, str):  # Vérifiez si l'entrée est une chaîne
        # Appliquer la lemmatisation sur chaque mot
        lemmatized_words = [lemma.lemmatize(word) for word in selftext.split()]
        # Joindre les mots lemmatisés en une chaîne
        return " ".join(lemmatized_words)
    return selftext

"""###### <font color='#1dd1a1'> Appliquer les différentes fonctions sur letitle


"""

# Appliquer la fonction remove_users
data['new_title'] = data.title.apply(func =remove_users)
# Appliquer la fonction remove_links
data['new_title'] = data.new_title.apply(func =remove_links)
# Appliquer la fonction email_address
data['new_title'] = data.new_title.apply(func =email_address)
# Appliquer la fonction remove_contraction
data['new_title'] = data.new_title.apply(func = contraction)
# Appliquer la fonction clean_html
data['new_title'] = data.new_title.apply(func =clean_html)
# Appliquer la fonction alpha_b
data['new_title'] = data.new_title.apply(func =alpha_b)
# Appliquer la fonction lower
data['new_title'] = data.new_title.apply(func =lower)
# Appliquer la fonction clean_space
data['new_title'] = data.new_title.apply(func =clean_space)
# Appliquer la fonction remove_stopwords
data['new_title'] = data.new_title.apply(func =remove_stopwords)
# Appliquer la fonction lem_sw
data['new_title'] = data.new_title.apply(func =lem_sw)

# Appliquer la fonction remove_users
data['new_selftext'] = data.selftext.apply(func =remove_users2)
# Appliquer la fonction remove_links
data['new_selftext'] = data.new_selftext.apply(func =remove_links2)
# Appliquer la fonction email_address
data['new_selftext'] = data.new_selftext.apply(func =email_address2)
# Appliquer la fonction remove_contraction
data['new_selftext'] = data.new_selftext.apply(func = contraction2)
# Appliquer la fonction clean_html
data['new_selftext'] = data.new_selftext.apply(func =clean_html2)
# Appliquer la fonction alpha_b
data['new_selftext'] = data.new_selftext.apply(func =alpha_b2)
# Appliquer la fonction lower
data['new_selftext'] = data.new_selftext.apply(func =lower2)
# Appliquer la fonction clean_space
data['new_selftext'] = data.new_selftext.apply(func =clean_space2)
# Appliquer la fonction remove_stopwords
data['new_selftext'] = data.new_selftext.apply(func =remove_stopwords2)
# Appliquer la fonction lem_sw
data['new_selftext'] = data.new_selftext.apply(func =lem_sw2)

# Afficher la ligne d'index 0
data['new_title'][0]

# Afficher la ligne d'index 1
data['new_selftext'][1]

data.isnull().sum()

data

columns_to_drop = ['title', 'selftext', 'created_utc', 'author', 'subreddit']
# Garder uniquement les colonnes existantes dans le DataFrame
columns_to_drop = [col for col in columns_to_drop if col in data.columns]

data = data.drop(columns=columns_to_drop)
data

"""### <font color='#00d2d3'> Vectorisation"""

# Importer les packages
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Nombre de mots
voc_size = len(set(" ".join(data.new_title).split()))
# voc_size2 = len(set(" ".join(data.new_selftext).split()))
# Initialiser le modèle Tokenizer
tokenizer = Tokenizer(num_words=voc_size, split=' ')
# Entrainer les données
tokenizer.fit_on_texts(data['new_title'].values)
# Vectoriser
X = tokenizer.texts_to_sequences(data['new_title'].values)
# Padding
X = pad_sequences(X)



"""### <font color='#00d2d3'> Sentiment Analysis avec Bi-directional LSTM"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout

# Dimension des vecteurs
emb_dim=100
model=Sequential()
model.add(Embedding(voc_size, emb_dim, input_length=X.shape[1]))
model.add(Bidirectional(LSTM(200, return_sequences=True)))
model.add(Dropout(0.4))
model.add(Bidirectional(LSTM(200)))
model.add(Dropout(0.4))
model.add(Dense(1,activation='sigmoid'))
# Compiler le modèle
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
# Afficher le summary
print(model.summary())